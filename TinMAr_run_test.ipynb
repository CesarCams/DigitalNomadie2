{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/martinmoureau/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/martinmoureau/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from torch import nn\n",
    "import random\n",
    "\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "data_path = \"data_intern/twitter-datasets/\"\n",
    "test_path = f\"{data_path}test_data.txt\"\n",
    "trainP_path = f\"{data_path}train_pos.txt\"\n",
    "trainN_path = f\"{data_path}train_neg.txt\"\n",
    "\n",
    "# Load and clean tweets\n",
    "def load_and_clean_test_tweets(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        tweets = file.readlines()\n",
    "    test_tweets = [re.sub(r\"^\\d+,\\s*\", \"\", tweet).strip() for tweet in tweets]\n",
    "    return test_tweets\n",
    "\n",
    "def load_tweets(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        tweets = file.readlines()\n",
    "    return tweets\n",
    "\n",
    "# Load datasets\n",
    "test_tweets = load_and_clean_test_tweets(test_path)\n",
    "trainP_tweets = load_tweets(trainP_path)\n",
    "trainN_tweets = load_tweets(trainN_path)\n",
    "\n",
    "# Combine labeled data\n",
    "labeled_tweets = [(tweet.strip(), 1) for tweet in trainP_tweets] + [(tweet.strip(), 0) for tweet in trainN_tweets]\n",
    "random.shuffle(labeled_tweets)\n",
    "\n",
    "# Smaller training\n",
    "# labeled_tweets = labeled_tweets[:20000]\n",
    "\n",
    "# Extract tweets and labels\n",
    "train_tweets = [tweet for tweet, label in labeled_tweets]\n",
    "train_labels = [label for tweet, label in labeled_tweets]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n",
      "/Users/martinmoureau/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting training embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Embeddings: 100%|██████████| 25000/25000 [33:59<00:00, 12.26it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False)\n",
    "base_model = AutoModel.from_pretrained(\"vinai/bertweet-base\")\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "base_model = base_model.to(device)\n",
    "\n",
    "# Dataset class for tokenization\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, tweets, tokenizer, max_length=128):\n",
    "        self.encodings = tokenizer(tweets, truncation=True, padding=True, max_length=max_length, return_tensors=\"pt\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.encodings[\"input_ids\"][idx],\n",
    "            \"attention_mask\": self.encodings[\"attention_mask\"][idx],\n",
    "        }\n",
    "\n",
    "# Function to extract embeddings\n",
    "def extract_embeddings(tweets, batch_size=8):\n",
    "    dataset = TweetDataset(tweets, tokenizer)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    embeddings = []\n",
    "\n",
    "    base_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Extracting Embeddings\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "            # Pass through model and extract CLS token\n",
    "            outputs = base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "            embeddings.append(cls_embeddings.cpu())  # Move to CPU to save MPS memory\n",
    "\n",
    "    return torch.cat(embeddings, dim=0).numpy()\n",
    "\n",
    "# Extract embeddings for training and test tweets\n",
    "print(\"Extracting training embeddings...\")\n",
    "train_embeddings = extract_embeddings(train_tweets, batch_size=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings and labels successfully saved to train_data.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert train_labels (list) to a NumPy array and ensure it is a column vector\n",
    "train_labels = np.array(train_labels).reshape(-1, 1)\n",
    "\n",
    "# Combine embeddings and labels into one array\n",
    "train_data = np.hstack((train_embeddings, train_labels))\n",
    "\n",
    "# Export to CSV\n",
    "np.savetxt(\n",
    "    \"/Users/martinmoureau/Documents/GitHub/DIGITALNOMADIE2/BERTweet_train_data.csv\",\n",
    "    train_data,\n",
    "    delimiter=\",\",\n",
    "    fmt=\"%.10f\"  # Format for floats (you can adjust precision)\n",
    ")\n",
    "print(\"Embeddings and labels successfully saved to BERTweet_train_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings and labels successfully loaded.\n",
      "Embeddings shape: (200000, 768)\n",
      "Labels shape: (200000,)\n"
     ]
    }
   ],
   "source": [
    "# Import from CSV\n",
    "train_data = np.loadtxt(\n",
    "    \"/Users/martinmoureau/Documents/GitHub/DIGITALNOMADIE2/BERTweet_train_data.csv\", \n",
    "    delimiter=\",\"\n",
    ")\n",
    "\n",
    "# Split back into embeddings and labels\n",
    "train_embeddings = train_data[:, :-1]  # All columns except the last\n",
    "train_labels = train_data[:, -1]       # Only the last column\n",
    "\n",
    "print(\"Embeddings and labels successfully loaded.\")\n",
    "print(\"Embeddings shape:\", train_embeddings.shape)\n",
    "print(\"Labels shape:\", train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(len(test_tweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;LogisticRegression<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(max_iter=1000)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train logistic regression classifier\n",
    "print(\"Training Logistic Regression...\")\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(train_embeddings, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Evaluation:\n",
      "Accuracy: 0.8725\n",
      "F1-Score: 0.8748\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "negative (-1)       0.89      0.85      0.87    100000\n",
      " positive (1)       0.86      0.89      0.87    100000\n",
      "\n",
      "     accuracy                           0.87    200000\n",
      "    macro avg       0.87      0.87      0.87    200000\n",
      " weighted avg       0.87      0.87      0.87    200000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "# Predict on the training set\n",
    "train_preds = log_reg.predict(train_embeddings)\n",
    "\n",
    "# Convert predictions: 1 for positive, -1 for negative\n",
    "train_preds = [1 if pred == 1 else -1 for pred in train_preds]\n",
    "train_labels_converted = [1 if label == 1 else -1 for label in train_labels]\n",
    "\n",
    "# Compute accuracy and F1-score\n",
    "train_accuracy = accuracy_score(train_labels_converted, train_preds)\n",
    "train_f1 = f1_score(train_labels_converted, train_preds, average=\"binary\", pos_label=1)\n",
    "\n",
    "# Print metrics\n",
    "print(\"Training Set Evaluation:\")\n",
    "print(f\"Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"F1-Score: {train_f1:.4f}\")\n",
    "\n",
    "# Optional: Print detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(train_labels_converted, train_preds, target_names=[\"negative (-1)\", \"positive (1)\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting test embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Embeddings: 100%|██████████| 1250/1250 [01:03<00:00, 19.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting sentiments on test data...\n",
      "finish creating csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from helpers import create_csv_submission\n",
    "\n",
    "print(\"Extracting test embeddings...\")\n",
    "test_embeddings = extract_embeddings(test_tweets, batch_size=8)\n",
    "\n",
    "# Predict on the test set\n",
    "print(\"Predicting sentiments on test data...\")\n",
    "test_preds = log_reg.predict(test_embeddings)\n",
    "\n",
    "# Convert predictions: 1 for positive, -1 for negative\n",
    "test_preds = [1 if pred == 1 else -1 for pred in test_preds]\n",
    "\n",
    "# Convert predictions: 1 for positive, -1 for negative\n",
    "test_preds = [1 if pred == 1 else -1 for pred in test_preds]\n",
    "\n",
    "# Create a list of tweet indices (1-based index)\n",
    "tweet_indices = np.arange(1, len(test_preds) + 1)\n",
    "\n",
    "# Save predictions using the provided function\n",
    "create_csv_submission(tweet_indices, test_preds, \"test_predictions.csv\")\n",
    "print('finish creating csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MLP Classifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 6250/6250 [00:21<00:00, 286.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.4351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 6250/6250 [00:21<00:00, 289.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Loss: 0.3563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 6250/6250 [00:21<00:00, 296.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Loss: 0.3341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 6250/6250 [00:21<00:00, 294.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Loss: 0.3228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 6250/6250 [00:23<00:00, 266.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Loss: 0.3149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 6250/6250 [00:25<00:00, 244.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Loss: 0.3098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 6250/6250 [00:21<00:00, 295.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Loss: 0.3059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 6250/6250 [00:21<00:00, 297.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Loss: 0.3024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 6250/6250 [00:20<00:00, 297.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Loss: 0.3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 6250/6250 [00:21<00:00, 292.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Loss: 0.2973\n",
      "MLP Classifier Training Complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the MLP Classifier\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_size=768, hidden_size=256, num_classes=2):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),  # Fully connected layer 1\n",
    "            nn.ReLU(),                          # Non-linear activation\n",
    "            nn.Dropout(0.3),                    # Dropout for regularization\n",
    "            nn.Linear(hidden_size, num_classes) # Output layer\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Convert embeddings and labels into tensors\n",
    "train_embeddings_tensor = torch.tensor(train_embeddings, dtype=torch.float32)\n",
    "train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)\n",
    "\n",
    "# Create DataLoader for training\n",
    "batch_size = 32\n",
    "train_dataset = TensorDataset(train_embeddings_tensor, train_labels_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize the MLP model, loss function, and optimizer\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "mlp = MLPClassifier(input_size=768, hidden_size=256, num_classes=2).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(mlp.parameters(), lr=1e-4)\n",
    "\n",
    "# Training loop\n",
    "print(\"Training MLP Classifier...\")\n",
    "num_epochs = 10\n",
    "\n",
    "mlp.train()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for batch_embeddings, batch_labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        batch_embeddings, batch_labels = batch_embeddings.to(device), batch_labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = mlp(batch_embeddings)\n",
    "        loss = criterion(outputs, batch_labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss/len(train_loader):.4f}\")\n",
    "\n",
    "print(\"MLP Classifier Training Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Evaluation:\n",
      "Accuracy: 0.8761\n",
      "F1-Score: 0.8807\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "negative (-1)       0.91      0.84      0.87    100000\n",
      " positive (1)       0.85      0.91      0.88    100000\n",
      "\n",
      "     accuracy                           0.88    200000\n",
      "    macro avg       0.88      0.88      0.88    200000\n",
      " weighted avg       0.88      0.88      0.88    200000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "# Set the MLP model to evaluation mode\n",
    "mlp.eval()\n",
    "\n",
    "# Predict on the training set\n",
    "with torch.no_grad():\n",
    "    train_embeddings_tensor = torch.tensor(train_embeddings, dtype=torch.float32).to(device)\n",
    "    outputs = mlp(train_embeddings_tensor)  # Forward pass\n",
    "    train_preds = torch.argmax(outputs, dim=1).cpu().numpy()  # Get predicted class indices\n",
    "\n",
    "# Convert predictions: 1 for positive, -1 for negative\n",
    "train_preds = [1 if pred == 1 else -1 for pred in train_preds]\n",
    "train_labels_converted = [1 if label == 1 else -1 for label in train_labels]\n",
    "\n",
    "# Compute accuracy and F1-score\n",
    "train_accuracy = accuracy_score(train_labels_converted, train_preds)\n",
    "train_f1 = f1_score(train_labels_converted, train_preds, average=\"binary\", pos_label=1)\n",
    "\n",
    "# Print metrics\n",
    "print(\"Training Set Evaluation:\")\n",
    "print(f\"Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"F1-Score: {train_f1:.4f}\")\n",
    "\n",
    "# Optional: Print detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(train_labels_converted, train_preds, target_names=[\"negative (-1)\", \"positive (1)\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting test embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Embeddings: 100%|██████████| 1250/1250 [01:03<00:00, 19.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting sentiments on test data...\n",
      "Finished creating 'test_predictions.csv'\n"
     ]
    }
   ],
   "source": [
    "from helpers import create_csv_submission\n",
    "import numpy as np\n",
    "\n",
    "# Extract test embeddings\n",
    "print(\"Extracting test embeddings...\")\n",
    "test_embeddings = extract_embeddings(test_tweets, batch_size=8)\n",
    "\n",
    "# Predict on the test set\n",
    "print(\"Predicting sentiments on test data...\")\n",
    "mlp.eval()  # Set MLP to evaluation mode\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_embeddings_tensor = torch.tensor(test_embeddings, dtype=torch.float32).to(device)\n",
    "    test_outputs = mlp(test_embeddings_tensor)  # Forward pass\n",
    "    test_preds = torch.argmax(test_outputs, dim=1).cpu().numpy()  # Get predicted class indices\n",
    "\n",
    "# Convert predictions: 1 for positive, -1 for negative\n",
    "test_preds = [1 if pred == 1 else -1 for pred in test_preds]\n",
    "\n",
    "# Create a list of tweet indices (1-based index)\n",
    "tweet_indices = np.arange(1, len(test_preds) + 1)\n",
    "\n",
    "# Save predictions using the provided function\n",
    "create_csv_submission(tweet_indices, test_preds, \"test_predictions.csv\")\n",
    "print(\"Finished creating 'test_predictions.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP2 classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 6250/6250 [00:08<00:00, 757.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] - Loss: 0.3754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 6250/6250 [00:06<00:00, 908.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10] - Loss: 0.3237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 6250/6250 [00:06<00:00, 918.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10] - Loss: 0.3111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 6250/6250 [00:06<00:00, 943.96it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10] - Loss: 0.3034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 6250/6250 [00:06<00:00, 907.65it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10] - Loss: 0.2976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 6250/6250 [00:06<00:00, 966.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10] - Loss: 0.2948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 6250/6250 [00:06<00:00, 898.14it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10] - Loss: 0.2901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 6250/6250 [00:06<00:00, 926.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10] - Loss: 0.2874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 6250/6250 [00:06<00:00, 913.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10] - Loss: 0.2855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 6250/6250 [00:06<00:00, 898.30it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10] - Loss: 0.2820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_size=768, hidden_size=256, num_classes=2, dropout=0.5):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size // 2)\n",
    "        self.output = nn.Linear(hidden_size // 2, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "# Convert embeddings and labels to tensors\n",
    "train_embeddings_tensor = torch.tensor(train_embeddings, dtype=torch.float32)\n",
    "train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 32\n",
    "train_dataset = TensorDataset(train_embeddings_tensor, train_labels_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "mlp2 = MLPClassifier(input_size=768, hidden_size=256, num_classes=2, dropout=0.3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(mlp2.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "num_epochs = 10\n",
    "mlp2.train()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for batch_embeddings, batch_labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        outputs = mlp2(batch_embeddings)\n",
    "        loss = criterion(outputs, batch_labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {epoch_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Evaluation:\n",
      "Accuracy: 0.8958\n",
      "F1-Score: 0.8961\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "negative (-1)       0.90      0.89      0.90    100000\n",
      " positive (1)       0.89      0.90      0.90    100000\n",
      "\n",
      "     accuracy                           0.90    200000\n",
      "    macro avg       0.90      0.90      0.90    200000\n",
      " weighted avg       0.90      0.90      0.90    200000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "# Set the MLP model to evaluation mode\n",
    "mlp2.eval()\n",
    "\n",
    "# Predict on the training set\n",
    "with torch.no_grad():\n",
    "    train_embeddings_tensor = torch.tensor(train_embeddings, dtype=torch.float32)\n",
    "    outputs = mlp2(train_embeddings_tensor)  # Forward pass\n",
    "    train_preds = torch.argmax(outputs, dim=1).numpy()  # Get predicted class indices\n",
    "\n",
    "# Convert predictions: 1 for positive, -1 for negative\n",
    "train_preds = [1 if pred == 1 else -1 for pred in train_preds]\n",
    "train_labels_converted = [1 if label == 1 else -1 for label in train_labels]\n",
    "\n",
    "# Compute accuracy and F1-score\n",
    "train_accuracy = accuracy_score(train_labels_converted, train_preds)\n",
    "train_f1 = f1_score(train_labels_converted, train_preds, average=\"binary\", pos_label=1)\n",
    "\n",
    "# Print metrics\n",
    "print(\"Training Set Evaluation:\")\n",
    "print(f\"Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"F1-Score: {train_f1:.4f}\")\n",
    "\n",
    "# Optional: Print detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(train_labels_converted, train_preds, target_names=[\"negative (-1)\", \"positive (1)\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting test embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Embeddings: 100%|██████████| 1250/1250 [01:03<00:00, 19.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting sentiments on test data...\n",
      "Finished creating 'test_predictions_mlp2.csv'\n"
     ]
    }
   ],
   "source": [
    "from helpers import create_csv_submission\n",
    "import numpy as np\n",
    "\n",
    "# Extract test embeddings\n",
    "print(\"Extracting test embeddings...\")\n",
    "test_embeddings = extract_embeddings(test_tweets, batch_size=8)\n",
    "\n",
    "# Predict on the test set using mlp2\n",
    "print(\"Predicting sentiments on test data...\")\n",
    "mlp2.eval()  # Set MLP to evaluation mode\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_embeddings_tensor = torch.tensor(test_embeddings, dtype=torch.float32)\n",
    "    test_outputs = mlp2(test_embeddings_tensor)  # Forward pass\n",
    "    test_preds = torch.argmax(test_outputs, dim=1).numpy()  # Get predicted class indices\n",
    "\n",
    "# Convert predictions: 1 for positive, -1 for negative\n",
    "test_preds = [1 if pred == 1 else -1 for pred in test_preds]\n",
    "\n",
    "# Create a list of tweet indices (1-based index)\n",
    "tweet_indices = np.arange(1, len(test_preds) + 1)\n",
    "\n",
    "# Save predictions using the provided function\n",
    "create_csv_submission(tweet_indices, test_preds, \"test_predictions_mlp2.csv\")\n",
    "print(\"Finished creating 'test_predictions_mlp2.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM classsifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardizing the embeddings...\n",
      "Applying PCA to reduce dimensionality...\n",
      "Original shape: (200000, 768), Reduced shape: (200000, 50)\n",
      "Training SVM Classifier...\n",
      "[LibSVM]......................."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "# Assuming the embeddings and labels are already loaded:\n",
    "# train_embeddings, train_labels_np, train_labels\n",
    "\n",
    "train_labels_np = np.array(train_labels)\n",
    "\n",
    "\n",
    "# Step 1: Standardize the embeddings before PCA\n",
    "print(\"Standardizing the embeddings...\")\n",
    "scaler = StandardScaler()\n",
    "train_embeddings_scaled = scaler.fit_transform(train_embeddings)\n",
    "\n",
    "# Step 2: Apply PCA to reduce dimensionality\n",
    "print(\"Applying PCA to reduce dimensionality...\")\n",
    "pca = PCA(n_components=50)  # Retain top 100 principal components\n",
    "train_embeddings_reduced = pca.fit_transform(train_embeddings_scaled)\n",
    "\n",
    "print(f\"Original shape: {train_embeddings.shape}, Reduced shape: {train_embeddings_reduced.shape}\")\n",
    "\n",
    "# Step 3: Train SVM Classifier\n",
    "print(\"Training SVM Classifier...\")\n",
    "svm_clf = SVC(kernel='rbf', C=1.0, gamma='scale', verbose=True)  # RBF kernel, default hyperparameters\n",
    "svm_clf.fit(train_embeddings_reduced, train_labels_np)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating SVM on Training Set...\n",
      "Training Set Evaluation:\n",
      "Accuracy: 0.7290\n",
      "F1-Score: 0.7617\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "negative (-1)       0.79      0.61      0.69       974\n",
      " positive (1)       0.69      0.84      0.76      1026\n",
      "\n",
      "     accuracy                           0.73      2000\n",
      "    macro avg       0.74      0.73      0.72      2000\n",
      " weighted avg       0.74      0.73      0.72      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "RED = 2000\n",
    "# Step 4: Evaluate on Training Set\n",
    "print(\"Evaluating SVM on Training Set...\")\n",
    "train_preds = svm_clf.predict(train_embeddings_reduced[:RED])\n",
    "\n",
    "# Convert predictions and labels to -1 and 1\n",
    "train_preds = [1 if pred == 1 else -1 for pred in train_preds]\n",
    "train_labels_converted = [1 if label == 1 else -1 for label in train_labels_np[:RED]]\n",
    "\n",
    "# Compute accuracy and F1-score\n",
    "train_accuracy = accuracy_score(train_labels_converted, train_preds)\n",
    "train_f1 = f1_score(train_labels_converted, train_preds, average=\"binary\", pos_label=1)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Training Set Evaluation:\")\n",
    "print(f\"Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"F1-Score: {train_f1:.4f}\")\n",
    "\n",
    "# Print detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(train_labels_converted, train_preds, target_names=[\"negative (-1)\", \"positive (1)\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on Training Set\n",
    "print(\"Evaluating SVM on Training Set...\")\n",
    "train_preds = svm_clf.predict(train_embeddings)\n",
    "\n",
    "# Convert predictions and labels to -1 and 1\n",
    "train_preds = [1 if pred == 1 else -1 for pred in train_preds]\n",
    "train_labels_converted = [1 if label == 1 else -1 for label in train_labels]\n",
    "\n",
    "# Compute accuracy and F1-score\n",
    "train_accuracy = accuracy_score(train_labels_converted, train_preds)\n",
    "train_f1 = f1_score(train_labels_converted, train_preds, average=\"binary\", pos_label=1)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Training Set Evaluation:\")\n",
    "print(f\"Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"F1-Score: {train_f1:.4f}\")\n",
    "\n",
    "# Print detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(train_labels_converted, train_preds, target_names=[\"negative (-1)\", \"positive (1)\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

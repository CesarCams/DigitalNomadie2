{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just getting started ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in vocab.txt: 101298\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1. Load the vocabulary (list of words)\n",
    "with open(\"vocab_cut.txt\", \"r\") as f:\n",
    "    words = [line.strip() for line in f]\n",
    "\n",
    "# Check the number of words in vocab.txt\n",
    "print(f\"Number of words in vocab.txt: {len(words)}\")\n",
    "\n",
    "# 2. Load the embedding matrix\n",
    "embedding_matrix = np.load(\"embeddings_transfo.npy\")\n",
    "\n",
    "# Check the shape of the embedding matrix\n",
    "#print(f\"Embedding matrix shape: {embedding_matrix.shape}\")  # Should be (101298, 20)\n",
    "\n",
    "# 3. Create the word-to-embedding dictionary\n",
    "glove_embeddings = {words[i]: embedding_matrix[i] for i in range(len(words))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet  label\n",
      "0           stupid ice coffee gave me a stomach ache     -1\n",
      "1  <user> ya ! seudah . plus decor is way prettie...      1\n",
      "2  <user> just watched your jenga game with <user...      1\n",
      "3  oh pandora , why do you have to be nice to me ...     -1\n",
      "4  ass grabbing while kissing > > > #somethingshe...      1\n",
      "                                               tweet  label\n",
      "0  1,sea doo pro sea scooter ( sports with the po...     -1\n",
      "1  2,<user> shucks well i work all week so now i ...     -1\n",
      "2          3,i cant stay away from bug thats my baby     -1\n",
      "3  4,<user> no ma'am ! ! ! lol im perfectly fine ...     -1\n",
      "4  5,whenever i fall asleep watching the tv , i a...     -1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file paths\n",
    "data_path = \"data/twitter-datasets/\"\n",
    "train_neg_path = f\"{data_path}train_neg_full.txt\"\n",
    "train_pos_path = f\"{data_path}train_pos_full.txt\"\n",
    "test_path = f\"{data_path}test_data.txt\"\n",
    "\n",
    "# Load negative tweets and assign a label of -1\n",
    "with open(train_neg_path, \"r\") as f:\n",
    "    neg_tweets = [(line.strip(), -1) for line in f]\n",
    "\n",
    "# Load positive tweets and assign a label of +1\n",
    "with open(train_pos_path, \"r\") as f:\n",
    "    pos_tweets = [(line.strip(), 1) for line in f]\n",
    "\n",
    "with open(test_path, \"r\") as f:\n",
    "    test_tweets = [(line.strip(), -1) for line in f]\n",
    "\n",
    "# Combine the positive and negative tweets into a single list\n",
    "tweets_with_labels = neg_tweets + pos_tweets\n",
    "\n",
    "# Optional: Shuffle the dataset (important for training)\n",
    "import random\n",
    "random.shuffle(tweets_with_labels)\n",
    "\n",
    "# Convert to a DataFrame for easy manipulation and viewing\n",
    "df = pd.DataFrame(tweets_with_labels, columns=[\"tweet\", \"label\"])\n",
    "df_test = pd.DataFrame(test_tweets, columns=[\"tweet\", \"label\"])\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df.head())\n",
    "print(df_test.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_embedding(tweet, glove_embeddings, embedding_dim=20):\n",
    "    words = tweet.split()  # Tokenize the tweet by splitting on whitespace\n",
    "    word_vectors = [glove_embeddings[word] for word in words if word in glove_embeddings]\n",
    "\n",
    "    if not word_vectors:\n",
    "        # If no words in the tweet have embeddings, return a zero vector\n",
    "        return np.zeros(embedding_dim)\n",
    "    \n",
    "    # Average the word vectors\n",
    "    avg_vector = np.mean(word_vectors, axis=0)\n",
    "    return avg_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (2500000, 384)\n",
      "Sample features: [[-0.04732432  0.01590263 -0.01478511 ...  0.02944557 -0.00037405\n",
      "   0.02391698]\n",
      " [-0.06798897  0.0254344   0.01065353 ...  0.01780745  0.03044034\n",
      "  -0.00491464]\n",
      " [-0.04491756  0.02407607 -0.00086558 ...  0.0276089   0.00562347\n",
      "   0.0198721 ]\n",
      " [-0.04566717  0.02825467 -0.00125924 ...  0.03640893  0.02461299\n",
      "   0.01075954]\n",
      " [-0.05077406  0.03024042  0.01428271 ...  0.03757446  0.01713703\n",
      "   0.01155091]]\n",
      "Labels: [-1  1  1 -1  1]\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 384  # Based on your embedding vector dimension\n",
    "df[\"feature\"] = df[\"tweet\"].apply(lambda tweet: get_average_embedding(tweet, glove_embeddings, embedding_dim))\n",
    "df.head()\n",
    "\n",
    "df_test[\"feature\"] = df_test[\"tweet\"].apply(lambda tweet: get_average_embedding(tweet, glove_embeddings, embedding_dim))\n",
    "feature_matrix_test = np.vstack(df_test[\"feature\"].values)\n",
    "# Convert the list of arrays in \"feature\" to a feature matrix for ML algorithms\n",
    "feature_matrix = np.vstack(df[\"feature\"].values)\n",
    "labels = df[\"label\"].values\n",
    "\n",
    "# Check the shape of the feature matrix and a sample of data\n",
    "print(\"Feature matrix shape:\", feature_matrix.shape)  # Should be (number_of_tweets, embedding_dim)\n",
    "print(\"Sample features:\", feature_matrix[:5])\n",
    "print(\"Labels:\", labels[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"feature_matrix_allMiniLM\", feature_matrix)\n",
    "np.save(\"labels_allMiniLM\",labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix = np.load(\"feature_matrix_allMiniLM.npy\")\n",
    "labels=np.load(\"labels_allMiniLM.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load your feature matrix and labels from previous steps\n",
    "# feature_matrix: the matrix of averaged embeddings for each tweet\n",
    "# labels: the corresponding labels (+1 for positive, -1 for negative)\n",
    "\n",
    "# Split the data into training and test sets (e.g., 80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_matrix, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the feature matrix\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Initialize the Linear Regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Convert predictions to binary labels (+1 or -1) by rounding to nearest integer\n",
    "y_pred_labels = np.where(y_pred >= 0, 1, -1)\n",
    "\n",
    "# Evaluate the model's performance using accuracy\n",
    "# accuracy = accuracy_score(y_test, y_pred_labels)\n",
    "# print(\"Test set accuracy:\", accuracy)\n",
    "\n",
    "# # Evaluate the model's performance using F1 score\n",
    "# f1 = f1_score(y_test, y_pred_labels)\n",
    "# print(\"Test set F1 score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.78639\n",
      "Test set F1 score: 0.7939118068727581\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the model's performance using accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred_labels)\n",
    "print(\"Test set accuracy:\", accuracy)\n",
    "\n",
    "#Evaluate the model's performance using F1 score\n",
    "f1 = f1_score(y_test, y_pred_labels)\n",
    "print(\"Test set F1 score:\", f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paul/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:697: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.841316\n",
      "Test set F1 score: 0.8438118860607492\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load your feature matrix and labels (replace with actual variables)\n",
    "# feature_matrix: averaged embeddings for each tweet\n",
    "# labels: corresponding labels (+1 for positive, -1 for negative)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    feature_matrix, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Normalize the feature matrix\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Initialize the MLP Classifier model with default parameters\n",
    "mlp = MLPClassifier(random_state=42)\n",
    "\n",
    "# Train the MLP Classifier\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = mlp.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Test set accuracy:\", accuracy)\n",
    "print(\"Test set F1 score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP torch (100 epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Epoch [10/100], Loss: 0.5626\n",
      "Epoch [20/100], Loss: 0.5044\n",
      "Epoch [30/100], Loss: 0.4732\n",
      "Epoch [40/100], Loss: 0.4537\n",
      "Epoch [50/100], Loss: 0.4409\n",
      "Epoch [60/100], Loss: 0.4320\n",
      "Epoch [70/100], Loss: 0.4254\n",
      "Epoch [80/100], Loss: 0.4202\n",
      "Epoch [90/100], Loss: 0.4159\n",
      "Epoch [100/100], Loss: 0.4122\n",
      "Test set accuracy: 0.8057\n",
      "Test set F1 score: 0.8101\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Vérifier si le GPU MPS est disponible\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "\n",
    "# Division en train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    feature_matrix, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Normalisation des données\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Conversion des données en tenseurs PyTorch\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "\n",
    "# Définir un modèle MLP\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),  # Couche cachée\n",
    "            nn.ReLU(),  # Fonction d'activation ReLU\n",
    "            nn.Linear(hidden_dim, output_dim),  # Couche de sortie\n",
    "            nn.Sigmoid()  # Fonction d'activation pour la classification binaire\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Initialiser le modèle, la fonction de perte et l'optimiseur\n",
    "input_dim = X_train.shape[1]\n",
    "hidden_dim = 50  # 50 neurones dans la couche cachée\n",
    "output_dim = 1   # Sortie binaire (sigmoïde)\n",
    "\n",
    "model = MLP(input_dim, hidden_dim, output_dim).to(device)\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Entraînement du modèle\n",
    "epochs = 100\n",
    "for epoch in tqdm(range(epochs), desc=\"Training Progress\", unit=\"epoch\"):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Prédiction et calcul de la perte\n",
    "    outputs = model(X_train).squeeze()\n",
    "    loss = criterion(outputs, (y_train + 1) / 2)  # Transformer -1/+1 en 0/1\n",
    "    \n",
    "    # Backpropagation et optimisation\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Évaluation du modèle\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_proba = model(X_test).squeeze()\n",
    "    y_pred = torch.where(y_pred_proba >= 0.5, 1.0, -1.0)  # Seuil à 0.5 pour binariser les prédictions\n",
    "\n",
    "# Conversion en NumPy pour sklearn\n",
    "y_test = y_test.cpu().numpy()\n",
    "y_pred = y_pred.cpu().numpy()\n",
    "\n",
    "# Évaluation avec accuracy et F1-score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Test set accuracy: {accuracy:.4f}\")\n",
    "print(f\"Test set F1 score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pour des essais réduction du nombre de tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original feature matrix shape: (2500000, 384)\n",
      "Reduced feature matrix shape: (2500, 384)\n",
      "Original labels shape: (2500000,)\n",
      "Reduced labels shape: (2500,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "sample_size = 2500\n",
    "selected_indices = np.random.choice(feature_matrix.shape[0], sample_size, replace=False)\n",
    "\n",
    "# Réduire feature_matrix et labels en fonction des indices sélectionnés\n",
    "reduced_feature_matrix = feature_matrix[selected_indices]\n",
    "reduced_labels = labels[selected_indices]\n",
    "\n",
    "# Afficher les formes des nouvelles matrices\n",
    "print(\"Original feature matrix shape:\", feature_matrix.shape)\n",
    "print(\"Reduced feature matrix shape:\", reduced_feature_matrix.shape)\n",
    "print(\"Original labels shape:\", labels.shape)\n",
    "print(\"Reduced labels shape:\", reduced_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.78632\n",
      "Test set F1 score: 0.7943882067664833\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load your feature matrix and labels from previous steps\n",
    "# feature_matrix: the matrix of averaged embeddings for each tweet\n",
    "# labels: the corresponding labels (+1 for positive, -1 for negative)\n",
    "\n",
    "# Split the data into training and test sets (e.g., 80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(reduced_feature_matrix, reduced_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the feature matrix\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Initialize the Linear Regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Convert predictions to binary labels (+1 or -1) by rounding to nearest integer\n",
    "y_pred_labels = np.where(y_pred >= 0, 1, -1)\n",
    "\n",
    "# Evaluate the model's performance using accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred_labels)\n",
    "print(\"Test set accuracy:\", accuracy)\n",
    "\n",
    "# Evaluate the model's performance using F1 score\n",
    "f1 = f1_score(y_test, y_pred_labels)\n",
    "print(\"Test set F1 score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 1000/1000 [00:08<00:00, 122.39epoch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.7866399884223938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm  # Barre de progression\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Split des données\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    reduced_feature_matrix, reduced_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Normalisation\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convertir en tenseurs torch et déplacer sur GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "\n",
    "# Modèle de régression linéaire\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# Initialiser et entraîner le modèle\n",
    "model = LinearRegressionModel(X_train.shape[1]).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Entraînement avec barre de progression\n",
    "epochs = 1000\n",
    "for epoch in tqdm(range(epochs), desc=\"Training Progress\", unit=\"epoch\"):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train).squeeze()\n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Prédiction\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test).squeeze()\n",
    "    y_pred_labels = torch.where(y_pred >= 0, torch.tensor(1.0).to(device), torch.tensor(-1.0).to(device))\n",
    "\n",
    "# Évaluation\n",
    "accuracy = (y_pred_labels == y_test).float().mean().item()\n",
    "print(\"Test set accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Epoch [10/100], Loss: 0.5595\n",
      "Epoch [20/100], Loss: 0.4993\n",
      "Epoch [30/100], Loss: 0.4694\n",
      "Epoch [40/100], Loss: 0.4508\n",
      "Epoch [50/100], Loss: 0.4387\n",
      "Epoch [60/100], Loss: 0.4301\n",
      "Epoch [70/100], Loss: 0.4236\n",
      "Epoch [80/100], Loss: 0.4183\n",
      "Epoch [90/100], Loss: 0.4137\n",
      "Epoch [100/100], Loss: 0.4095\n",
      "Test set accuracy: 0.8052\n",
      "Test set F1 score: 0.8111\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Vérifier si le GPU MPS est disponible\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "\n",
    "# Division en train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    reduced_feature_matrix, reduced_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Normalisation des données\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Conversion des données en tenseurs PyTorch\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "\n",
    "# Définir un modèle MLP\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),  # Couche cachée\n",
    "            nn.ReLU(),  # Fonction d'activation ReLU\n",
    "            nn.Linear(hidden_dim, output_dim),  # Couche de sortie\n",
    "            nn.Sigmoid()  # Fonction d'activation pour la classification binaire\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Initialiser le modèle, la fonction de perte et l'optimiseur\n",
    "input_dim = X_train.shape[1]\n",
    "hidden_dim = 50  # 50 neurones dans la couche cachée\n",
    "output_dim = 1   # Sortie binaire (sigmoïde)\n",
    "\n",
    "model = MLP(input_dim, hidden_dim, output_dim).to(device)\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Entraînement du modèle\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Prédiction et calcul de la perte\n",
    "    outputs = model(X_train).squeeze()\n",
    "    loss = criterion(outputs, (y_train + 1) / 2)  # Transformer -1/+1 en 0/1\n",
    "    \n",
    "    # Backpropagation et optimisation\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Évaluation du modèle\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_proba = model(X_test).squeeze()\n",
    "    y_pred = torch.where(y_pred_proba >= 0.5, 1.0, -1.0)  # Seuil à 0.5 pour binariser les prédictions\n",
    "\n",
    "# Conversion en NumPy pour sklearn\n",
    "y_test = y_test.cpu().numpy()\n",
    "y_pred = y_pred.cpu().numpy()\n",
    "\n",
    "# Évaluation avec accuracy et F1-score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Test set accuracy: {accuracy:.4f}\")\n",
    "print(f\"Test set F1 score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.79286\n",
      "Test set F1 score: 0.7968139995683989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paul/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load your feature matrix and labels (replace with actual variables)\n",
    "# feature_matrix: averaged embeddings for each tweet\n",
    "# labels: corresponding labels (+1 for positive, -1 for negative)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    reduced_feature_matrix, reduced_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Normalize the feature matrix\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Initialize the MLP Classifier model with default parameters\n",
    "mlp = MLPClassifier(random_state=42)\n",
    "\n",
    "# Train the MLP Classifier\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = mlp.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Test set accuracy:\", accuracy)\n",
    "print(\"Test set F1 score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = np.arange(1, len(y_pred_labels) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import create_csv_submission\n",
    "create_csv_submission(ids, y_pred_labels, \"submission_embed_transfo1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.metrics import accuracy_score, f1_score
from sklearn.preprocessing import StandardScaler
from itertools import product
from tqdm import tqdm

#Selection of the model to encode the tweets ahead of training the classifier : 

#model_name = "sentence-transformers/all-MiniLM-L6-v2"
model_name = "vinai/bertweet-base" #This one gave the best results
#model_name = "distilbert-base-uncased"
#model_name = "cardiffnlp/twitter-roberta-base-sentiment"

#Selection of the classifier to use : 

classifier = "LogisticRegression" 
#classifier = "LinearRegression"
#classifier = "MLPClassifier" #This one gave the best results

features_np = np.load(f"features_all_{model_name.split("/")[-1]}.npy")
features = torch.tensor(features_np, dtype=torch.float32)
labels = np.load(f"labels_all_{model_name.split("/")[-1]}.npy")

print("Embeddings generated by the model: ",model_name)

X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)


scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)


if classifier == "LogisticRegression":
    param_grid = {
        'C': [0.01,0.1, 1, 10,100,1000],  
        'penalty': ['l2'],  
    }

    logistic_model = LogisticRegression(max_iter=1000)

    grid_search = GridSearchCV(estimator=logistic_model, param_grid=param_grid, scoring='f1', cv=3, verbose=2, n_jobs=-1)

    grid_search.fit(X_train, y_train)

    best_model = grid_search.best_estimator_
    y_pred = best_model.predict(X_test)

    accuracy = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)

    print("Optimal parameters and results for logistic regression : ")
    print("Best Parameters: ", grid_search.best_params_)
    print("Test set F1 Score: ", f1)
    print("Test set accuracy : ", accuracy)

elif classifier == "LinearRegression":
    # Closed form solution, no grid search needed
    model = LinearRegression()
    model.fit(X_train, y_train)

    y_pred = model.predict(X_test)

    y_pred_labels = np.where(y_pred >= 0, 1, -1)

    accuracy = accuracy_score(y_test, y_pred_labels)
    f1 = f1_score(y_test, y_pred_labels)
    print("Results for linear regression : ")
    print("Test set F1 score:", f1)
    print("Test set accuracy:", accuracy)
    

elif classifier == "MLPClassifier":
    X_train = torch.tensor(X_train, dtype=torch.float32)
    X_test = torch.tensor(X_test, dtype=torch.float32)
    y_train = torch.tensor(y_train, dtype=torch.float32)
    y_test = torch.tensor(y_test, dtype=torch.float32)

    class MLP(nn.Module):
        def __init__(self, input_dim, hidden_dim, output_dim):
            super(MLP, self).__init__()
            self.model = nn.Sequential(
                nn.Linear(input_dim, hidden_dim),  
                nn.ReLU(),  
                nn.Linear(hidden_dim, output_dim),  
                nn.Sigmoid()  
            )

        def forward(self, x):
            return self.model(x)

    hidden_layer_sizes = [50, 100, 200]  
    learning_rates = [0.00001, 0.001, 0.1]  
    epoch_values = [100]  

    best_f1 = 0
    best_params = {}

    # Grid Search
    for hidden_dim, lr, epochs in product(hidden_layer_sizes, learning_rates, epoch_values):
        print(f"Testing configuration: Hidden Layer Size={hidden_dim}, Learning Rate={lr}, Epochs={epochs}")

        input_dim = X_train.shape[1]
        output_dim = 1   

        #device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        device = "mps" if torch.backends.mps.is_available() else "cpu" #Uncomment to run on GPU for Mac

        model = MLP(input_dim, hidden_dim, output_dim).to(device)
        criterion = nn.BCELoss()  # Binary Cross-Entropy Loss
        optimizer = optim.Adam(model.parameters(), lr=lr)

        X_train_device = X_train.to(device)
        X_test_device = X_test.to(device)
        y_train_device = y_train.to(device)
        y_test_device = y_test.to(device)

        for epoch in tqdm(range(epochs), desc=f"Training Progress (Hidden={hidden_dim}, LR={lr}, Epochs={epochs})", unit="epoch"):
            model.train()
            optimizer.zero_grad()

            outputs = model(X_train_device).squeeze()
            loss = criterion(outputs, (y_train_device + 1) / 2)  

            loss.backward()
            optimizer.step()

        model.eval()
        with torch.no_grad():
            y_pred_proba = model(X_test_device).squeeze()
            y_pred = torch.where(y_pred_proba >= 0.5, 1.0, -1.0)  
        y_test_np = y_test_device.cpu().numpy()
        y_pred_np = y_pred.cpu().numpy()

        accuracy = accuracy_score(y_test_np, y_pred_np)
        f1 = f1_score(y_test_np, y_pred_np)

        print(f"Configuration: Hidden={hidden_dim}, LR={lr}, Epochs={epochs} | Accuracy={accuracy:.4f}, F1={f1:.4f}")

        if f1 > best_f1:
            best_f1 = f1
            acc = accuracy
            best_params = {
                "hidden_dim": hidden_dim,
                "learning_rate": lr,
                "epochs": epochs
            }

    print("Optimal parameters and results for MLP classifier : ")
    print("Best Parameters:", best_params)
    print("Test set F1 Score:", best_f1)
    print("Test set accuracy:", acc)
    



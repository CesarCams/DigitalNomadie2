{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just getting started ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in vocab.txt: 101298\n",
      "Embedding matrix shape: (101298, 20)\n",
      "Embedding for 'dog': [ 0.08723993  0.198662   -0.60317519  0.42002618  0.09527151  0.44971682\n",
      " -0.16778793 -0.24183066  0.53596791 -0.32699965  1.05951738  0.68060991\n",
      "  0.45617437 -0.47227478 -0.39513353  0.35734509 -0.34283911  0.17890524\n",
      "  0.33702634 -0.10559697]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1. Load the vocabulary (list of words)\n",
    "with open(\"vocab_cut.txt\", \"r\") as f:\n",
    "    words = [line.strip() for line in f]\n",
    "\n",
    "# Check the number of words in vocab.txt\n",
    "print(f\"Number of words in vocab.txt: {len(words)}\")\n",
    "\n",
    "# 2. Load the embedding matrix\n",
    "embedding_matrix = np.load(\"embeddings.npy\")\n",
    "\n",
    "# Check the shape of the embedding matrix\n",
    "print(f\"Embedding matrix shape: {embedding_matrix.shape}\")  # Should be (101298, 20)\n",
    "\n",
    "# 3. Create the word-to-embedding dictionary\n",
    "glove_embeddings = {words[i]: embedding_matrix[i] for i in range(len(words))}\n",
    "\n",
    "# Example: Access the embedding for the word \"dog\"\n",
    "word_embedding = glove_embeddings.get(\"dog\")  # Replace \"dog\" with any word\n",
    "if word_embedding is not None:\n",
    "    print(f\"Embedding for 'dog': {word_embedding}\")\n",
    "else:\n",
    "    print(\"Word not found in vocabulary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet  label\n",
      "0  i wonder who are you talking about ... it's de...     -1\n",
      "1                 glad to b off dnt feel good at all     -1\n",
      "2  loving my peeps on twitter protesting the fuck...      1\n",
      "3  i love you <user> wait for tonight's clip of b...      1\n",
      "4  <user> you and <user> will make the cutest cou...      1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file paths\n",
    "data_path = \"data/twitter-datasets/\"\n",
    "train_neg_path = f\"{data_path}train_neg.txt\"\n",
    "train_pos_path = f\"{data_path}train_pos.txt\"\n",
    "\n",
    "# Load negative tweets and assign a label of -1\n",
    "with open(train_neg_path, \"r\") as f:\n",
    "    neg_tweets = [(line.strip(), -1) for line in f]\n",
    "\n",
    "# Load positive tweets and assign a label of +1\n",
    "with open(train_pos_path, \"r\") as f:\n",
    "    pos_tweets = [(line.strip(), 1) for line in f]\n",
    "\n",
    "# Combine the positive and negative tweets into a single list\n",
    "tweets_with_labels = neg_tweets + pos_tweets\n",
    "\n",
    "# Optional: Shuffle the dataset (important for training)\n",
    "import random\n",
    "random.shuffle(tweets_with_labels)\n",
    "\n",
    "# Convert to a DataFrame for easy manipulation and viewing\n",
    "df = pd.DataFrame(tweets_with_labels, columns=[\"tweet\", \"label\"])\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_embedding(tweet, glove_embeddings, embedding_dim=20):\n",
    "    words = tweet.split()  # Tokenize the tweet by splitting on whitespace\n",
    "    word_vectors = [glove_embeddings[word] for word in words if word in glove_embeddings]\n",
    "\n",
    "    if not word_vectors:\n",
    "        # If no words in the tweet have embeddings, return a zero vector\n",
    "        return np.zeros(embedding_dim)\n",
    "    \n",
    "    # Average the word vectors\n",
    "    avg_vector = np.mean(word_vectors, axis=0)\n",
    "    return avg_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (200000, 20)\n",
      "Sample features: [[ 3.46951193e-01 -5.39721850e-02 -2.85347753e-01  4.15248704e-01\n",
      "  -3.09205731e-01  2.89634001e-01  1.10817865e-01 -3.14039857e-01\n",
      "  -8.43951729e-02  1.26812239e-01  8.54011491e-02  2.25696590e-01\n",
      "   4.67463946e-01 -4.39469099e-01  8.31066792e-02  5.91286910e-01\n",
      "   1.81714505e-01 -9.58502088e-02  3.28682083e-01 -1.04378108e-02]\n",
      " [ 4.42388857e-01  2.16337682e-01 -2.50358366e-01  5.02090672e-01\n",
      "  -1.27068211e-01  2.12625968e-01  2.76913617e-02 -4.81798412e-01\n",
      "   5.33096252e-02  1.19584558e-01  1.85556101e-01  3.08845606e-01\n",
      "   5.26943232e-01 -5.60846487e-01  4.27428091e-02  4.92746303e-01\n",
      "   3.04177767e-01 -1.02890440e-01  4.73283879e-01 -1.35745022e-01]\n",
      " [ 1.42379915e-01  2.19552371e-02 -1.68202581e-01  2.07059272e-01\n",
      "  -1.24985767e-01  1.21580990e-01 -9.75649833e-02 -1.92837410e-01\n",
      "  -8.10305916e-02 -3.14825180e-02  5.76040921e-02  5.02743517e-02\n",
      "   2.93349135e-01 -3.24897987e-01 -1.11397981e-01  4.56748483e-01\n",
      "   1.39910613e-01  3.30899752e-02  3.44992081e-01 -1.58958290e-01]\n",
      " [ 1.39538470e-01  1.46620853e-01 -1.43626846e-01  3.69963194e-01\n",
      "  -4.30793409e-04  2.26508606e-01  1.22744262e-01 -3.09625179e-01\n",
      "  -1.54516808e-01  2.18153182e-01  8.57797883e-02  2.65013884e-01\n",
      "   3.29825384e-01 -1.81601995e-01  2.18268662e-02  2.26158502e-01\n",
      "   2.51904868e-01 -6.63344948e-02  5.37713375e-01 -1.20212033e-01]\n",
      " [ 1.35528368e-01  9.05646561e-02 -3.26714460e-01  3.22415871e-01\n",
      "   1.45510249e-02  2.27001974e-01  4.04269192e-01 -3.45036411e-01\n",
      "   7.71207763e-02  2.54248136e-02 -2.30805273e-01  4.11030894e-01\n",
      "   2.09945526e-01 -3.00481080e-01  7.89017909e-02  4.17581943e-01\n",
      "   2.72205790e-01 -3.86624812e-02  2.92313502e-01  2.63827971e-02]]\n",
      "Labels: [-1 -1  1  1  1]\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 20  # Based on your embedding vector dimension\n",
    "df[\"feature\"] = df[\"tweet\"].apply(lambda tweet: get_average_embedding(tweet, glove_embeddings, embedding_dim))\n",
    "df.head()\n",
    "# Convert the list of arrays in \"feature\" to a feature matrix for ML algorithms\n",
    "feature_matrix = np.vstack(df[\"feature\"].values)\n",
    "labels = df[\"label\"].values\n",
    "\n",
    "# Check the shape of the feature matrix and a sample of data\n",
    "print(\"Feature matrix shape:\", feature_matrix.shape)  # Should be (number_of_tweets, embedding_dim)\n",
    "print(\"Sample features:\", feature_matrix[:5])\n",
    "print(\"Labels:\", labels[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.574325\n",
      "Test set F1 score: 0.5966217336713179\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load your feature matrix and labels from previous steps\n",
    "# feature_matrix: the matrix of averaged embeddings for each tweet\n",
    "# labels: the corresponding labels (+1 for positive, -1 for negative)\n",
    "\n",
    "# Split the data into training and test sets (e.g., 80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_matrix, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the feature matrix\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Initialize the Linear Regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Convert predictions to binary labels (+1 or -1) by rounding to nearest integer\n",
    "y_pred_labels = np.where(y_pred >= 0, 1, -1)\n",
    "\n",
    "# Evaluate the model's performance using accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred_labels)\n",
    "print(\"Test set accuracy:\", accuracy)\n",
    "\n",
    "# Evaluate the model's performance using F1 score\n",
    "f1 = f1_score(y_test, y_pred_labels)\n",
    "print(\"Test set F1 score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
